calculator {

    aws.region=${AWS_REGION}

    tenantId=${TENANT_ID}
    environment=${ENVIRONMENT}

    temp.location=/tmp
    temp.location=${?TEMP_DIR}

    calculations.functionName=${CALCULATIONS_FUNCTION_NAME}
    referenceDatasets.functionName=${REFERENCEDATASETS_FUNCTION_NAME}
    impacts.functionName=${IMPACTS_FUNCTION_NAME}
    accessManagement.functionName=${ACCESS_MANAGEMENT_FUNCTION_NAME}
    resourceMappingTableName=${RESOURCE_MAPPING_TABLE_NAME}
    // default to empty string if caml endpoint is not specified
    caml.inferenceEndpointName=""
    caml.inferenceEndpointName=${?CAML_INFERENCE_ENDPOINT_NAME}


    audits.kinesis.dataStreamName=${AUDIT_DATA_STREAM_NAME}
    audits.kinesis.baseBackOffInMillis=500
    audits.kinesis.bufferFullWaitTimeoutInMillis=100
    audits.kinesis.bufferTimeoutBetweenFlushes=50
    audits.kinesis.bufferTimeoutInMillis=300000
    audits.kinesis.maxBackOffInMillis=10000
    audits.kinesis.maxBufferSize=100
    audits.kinesis.maxOperationTimeoutInMillis=300000
    audits.kinesis.maxPutRecordBatchBytes=104448   # us-east-1 / us-west-2 / eu-west-1 = 4194304 else 104448
    audits.kinesis.numberOfRetries=10


    activity.sqs.baseBackOffInMillis=500
    activity.sqs.bufferFullWaitTimeoutInMillis=100
    activity.sqs.bufferTimeoutBetweenFlushes=50
    activity.sqs.bufferTimeoutInMillis=300000
    activity.sqs.maxBackOffInMillis=10000
    activity.sqs.maxBufferSize=100
    // intentionally set a little lower than the 256 KiB limit:
    activity.sqs.maxMessageSize=250000
    activity.sqs.maxOperationTimeoutInMillis=300000
    activity.sqs.numberOfRetries=10
    activity.sqs.maxPutObjectsCount=100
    activity.sqs.queueUrl=${ACTIVITY_QUEUE_URL}

    upload.s3.bucket=${BUCKET_NAME}
    upload.s3.errors.key="pipelines/<pipelineId>/executions/<executionId>/errors-<chunkNo>.txt"
    upload.s3.audit.key="pipelines/<pipelineId>/executions/<executionId>/audits/<auditId>.json"
    upload.s3.activities.key="pipelines/<pipelineId>/executions/<executionId>/output/"
    upload.s3.activities.name="activities"
    upload.s3.activityValues.name="activity-values"
    upload.s3.groups.key="pipelines/<pipelineId>/executions/<executionId>/groups/<chunkNo>.txt"
    executor.threads=10

    bulkInsert.rowThreshold=10
}
