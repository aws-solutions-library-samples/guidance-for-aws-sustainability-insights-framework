calculator {

    aws.region=${AWS_REGION}

    tenantId=${TENANT_ID}
    environment=${ENVIRONMENT}

    temp.location=/tmp
    temp.location=${?TEMP_DIR}

    calculations.functionName=${CALCULATIONS_FUNCTION_NAME}
    referenceDatasets.functionName=${REFERENCEDATASETS_FUNCTION_NAME}
    impacts.functionName=${IMPACTS_FUNCTION_NAME}
    users.functionName=${USERS_FUNCTION_NAME}
    resourceMappingTableName=${RESOURCE_MAPPING_TABLE_NAME}

    audits.sqs.baseBackOffInMillis=500
    audits.sqs.bufferFullWaitTimeoutInMillis=100
    audits.sqs.bufferTimeoutBetweenFlushes=50
    audits.sqs.bufferTimeoutInMillis=300000
    audits.sqs.maxBackOffInMillis=10000
    audits.sqs.maxBufferSize=100
    // intentionally set a little lower than the 256 KiB limit:
    audits.sqs.maxMessageSize=250000
    audits.sqs.maxOperationTimeoutInMillis=300000
    audits.sqs.numberOfRetries=10
    audits.sqs.maxPutObjectsCount=100
    audits.sqs.queueUrl=${AUDIT_QUEUE_URL}

    activity.sqs.baseBackOffInMillis=500
    activity.sqs.bufferFullWaitTimeoutInMillis=100
    activity.sqs.bufferTimeoutBetweenFlushes=50
    activity.sqs.bufferTimeoutInMillis=300000
    activity.sqs.maxBackOffInMillis=10000
    activity.sqs.maxBufferSize=100
    // intentionally set a little lower than the 256 KiB limit:
    activity.sqs.maxMessageSize=250000
    activity.sqs.maxOperationTimeoutInMillis=300000
    activity.sqs.numberOfRetries=10
    activity.sqs.maxPutObjectsCount=100
    activity.sqs.queueUrl=${ACTIVITY_QUEUE_URL}

    upload.s3.bucket=${BUCKET_NAME}
    upload.s3.errors.key="pipelines/<pipelineId>/executions/<executionId>/errors-<chunkNo>.txt"
    upload.s3.audit.key="pipelines/<pipelineId>/executions/<executionId>/audits/<auditId>.json"
	upload.s3.activities.key="pipelines/<pipelineId>/executions/<executionId>/output/"
	upload.s3.activities.name="activities"
	upload.s3.activityValues.name="activity-values"
    executor.threads=10

	bulkInsert.rowThreshold=10
}
